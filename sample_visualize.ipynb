{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample and Visualize Samples From a Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from operator import is_\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from improved_diffusion import dist_util\n",
    "from improved_diffusion.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    args_to_dict,\n",
    "    str2bool,\n",
    ")\n",
    "from improved_diffusion.test_util import get_model_results_path, get_eval_run_identifier, Protect\n",
    "from improved_diffusion.sampling_schemes import sampling_schemes\n",
    "from improved_diffusion.video_datasets import get_eval_dataset, eval_dataset_configs\n",
    "\n",
    "\n",
    "@th.no_grad()\n",
    "def sample_video(args, model, diffusion, batch, just_get_indices=False):\n",
    "    \"\"\"\n",
    "    batch has a shape of BxTxCxHxW where\n",
    "    B: batch size\n",
    "    T: video length\n",
    "    CxWxH: image size\n",
    "    \"\"\"\n",
    "    B, T, C, H, W = batch.shape\n",
    "    samples = th.zeros_like(batch)\n",
    "    samples[:, :args.n_obs] = batch[:, :args.n_obs]\n",
    "\n",
    "    # Observation-level samples\n",
    "    visualized_samples = None\n",
    "\n",
    "    # Intilise sampling scheme\n",
    "    optimal_schedule_path = None if args.optimality is None else args.eval_dir / \"optimal_schedule.pt\"\n",
    "    frame_indices_iterator = iter(sampling_schemes[args.sampling_scheme](\n",
    "        video_length=T, num_obs=args.n_obs,\n",
    "        max_frames=args.max_frames, step_size=args.max_latent_frames,\n",
    "        optimal_schedule_path=optimal_schedule_path,\n",
    "    ))\n",
    "\n",
    "    indices_used = []\n",
    "    while True:\n",
    "        frame_indices_iterator.set_videos(samples.to(args.device))  # ignored for non-adaptive sampling schemes\n",
    "        try:\n",
    "            obs_frame_indices, latent_frame_indices = next(frame_indices_iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        print(f\"Conditioning on {sorted(obs_frame_indices)} frames, predicting {sorted(latent_frame_indices)}.\")\n",
    "        # Prepare network's input\n",
    "        frame_indices = th.cat([th.tensor(obs_frame_indices), th.tensor(latent_frame_indices)], dim=1).long()\n",
    "        x0 = th.stack([samples[i, fi] for i, fi in enumerate(frame_indices)], dim=0).clone()\n",
    "        obs_mask = th.cat([th.ones_like(th.tensor(obs_frame_indices)),\n",
    "                              th.zeros_like(th.tensor(latent_frame_indices))], dim=1).view(B, -1, 1, 1, 1).float()\n",
    "        latent_mask = 1 - obs_mask\n",
    "        if just_get_indices:\n",
    "            local_samples = th.stack([batch[i, ind] for i, ind in enumerate(frame_indices)])\n",
    "        else:\n",
    "            # Prepare masks\n",
    "            print(f\"{'Frame indices':20}: {frame_indices[0].cpu().numpy()}.\")\n",
    "            print(f\"{'Observation mask':20}: {obs_mask[0].cpu().int().numpy().squeeze()}\")\n",
    "            print(f\"{'Latent mask':20}: {latent_mask[0].cpu().int().numpy().squeeze()}\")\n",
    "            print(\"-\" * 40)\n",
    "            # Move tensors to the correct device\n",
    "            x0, obs_mask, latent_mask, frame_indices = (t.to(args.device) for t in [x0, obs_mask, latent_mask, frame_indices])\n",
    "            # Run the network\n",
    "            sampler, *sampler_args = args.sampler.split('-')\n",
    "            if sampler == \"ddpm\":\n",
    "                sample_func = diffusion.p_sample_loop\n",
    "                sampler_kwargs = {}\n",
    "            elif sampler == \"ddim\":\n",
    "                sample_func = diffusion.ddim_sample_loop\n",
    "                sampler_kwargs = {}\n",
    "            elif sampler == \"heun\":\n",
    "                sample_func = diffusion.heun_sample\n",
    "                (S_churn, S_max, S_min, S_noise,\n",
    "                 sigma_max, sigma_min, rho, num_steps) = sampler_args\n",
    "                sampler_kwargs = dict(\n",
    "                    S_churn=float(S_churn), S_max=float(S_max),\n",
    "                    S_min=float(S_min), S_noise=float(S_noise),\n",
    "                    sigma_max=float(sigma_max), sigma_min=float(sigma_min),\n",
    "                    rho=int(rho), num_steps=int(num_steps)\n",
    "                )\n",
    "            print('sample_func', sample_func)\n",
    "            local_samples, _ = sample_func(\n",
    "                model, x0.shape, clip_denoised=args.clip_denoised,\n",
    "                model_kwargs=dict(frame_indices=frame_indices,\n",
    "                                  x0=x0,\n",
    "                                  obs_mask=obs_mask,\n",
    "                                  latent_mask=latent_mask),\n",
    "                latent_mask=latent_mask,\n",
    "                return_attn_weights=False,\n",
    "                decode_chunk_size=args.decode_chunk_size,\n",
    "                **sampler_kwargs,\n",
    "            )\n",
    "\n",
    "            if isinstance(local_samples, tuple):\n",
    "                # Edge case: Encoded sample\n",
    "                visualized_local_samples = local_samples[1]\n",
    "                local_samples = local_samples[0].to(x0.dtype)\n",
    "            else:\n",
    "                # No encoded samples\n",
    "                visualized_local_samples = local_samples.to(x0.dtype)\n",
    "\n",
    "            if visualized_samples is None:\n",
    "                if local_samples.shape == visualized_local_samples.shape:\n",
    "                    decoded_obs_batch = batch[:, args.n_obs].to(batch.device)\n",
    "                else:\n",
    "                    decoded_obs_batch = diffusion.decode(batch[:, :args.n_obs].to(th.float16),\n",
    "                                                         chunk_size=args.decode_chunk_size).to(batch.device)\n",
    "                C_d, H_d, W_d = decoded_obs_batch.shape[2:]\n",
    "                visualized_samples = th.zeros(B, T, *decoded_obs_batch.shape[2:]).to(batch.device).to(batch.dtype)\n",
    "                visualized_samples[:, :args.n_obs] = decoded_obs_batch\n",
    "\n",
    "            print('local samples', local_samples.min(), local_samples.max())\n",
    "\n",
    "        # Fill in the generated frames\n",
    "        for i, li in enumerate(latent_frame_indices):\n",
    "            samples[i, li] = local_samples[i, -len(li):].cpu().to(samples.dtype)\n",
    "            visualized_samples[i, li] = visualized_local_samples[i, -len(li):].cpu().to(visualized_local_samples.dtype)\n",
    "        indices_used.append((obs_frame_indices, latent_frame_indices))\n",
    "    return visualized_samples, samples, indices_used\n",
    "\n",
    "\n",
    "def generate_samples(args, model, diffusion, dataset, samples_prefix, seed=0, cache_samples=False):\n",
    "    all_samples, all_unprocessed_samples = [], []\n",
    "    not_done = list(args.indices)\n",
    "    while len(not_done) > 0:\n",
    "        batch_indices = not_done[:args.batch_size]\n",
    "        not_done = not_done[args.batch_size:]\n",
    "        output_filenames = [args.eval_dir / samples_prefix / f\"sample_{i:04d}-{seed}.npy\" for i in batch_indices]\n",
    "        todo = [not p.exists() for p in output_filenames]\n",
    "        if not any(todo) and cache_samples:\n",
    "            samples = []\n",
    "            print(f\"Reading from cache: {output_filenames}\")\n",
    "            all_samples.extend([np.load(fname) for fname in output_filenames])\n",
    "            continue\n",
    "        batch = th.stack([dataset[i][0] for i in batch_indices])\n",
    "        samples, unprocessed_samples, _ = sample_video(args, model, diffusion, batch)\n",
    "        drange = [-1, 1]\n",
    "\n",
    "        samples = (samples.clamp(*drange).numpy() - drange[0]) / (drange[1] - drange[0]) * 255\n",
    "        samples = samples.astype(np.uint8)\n",
    "        for i in range(len(batch_indices)):\n",
    "            if todo[i]:\n",
    "                np.save(output_filenames[i], samples[i])\n",
    "                print(f\"*** Saved {output_filenames[i]} ***\")\n",
    "        all_samples.extend(samples)\n",
    "        all_unprocessed_samples.extend(unprocessed_samples)\n",
    "    return all_samples, all_unprocessed_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--checkpoint_path\", type=str, default=\"checkpoints/1suvj1hv/ema_0.9999_140000.pt\")\n",
    "parser.add_argument(\"--out_dir\", type=str, default=\"tmp\")\n",
    "parser.add_argument(\"--eval_dir\", type=str, default=None)\n",
    "parser.add_argument(\"--num_sampled_videos\", type=int, default=None,\n",
    "                    help=\"Total number of samples (default: args.stop_index-args.start_index)\")\n",
    "parser.add_argument(\"--sampling_scheme\", default=\"autoreg\", choices=sampling_schemes.keys())\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "parser.add_argument(\"--n_obs\", type=int, default=10, help=\"Number of observed frames at the beginning of the video. The rest are sampled.\")\n",
    "parser.add_argument(\"--T\", type=int, default=20, help=\"Length of the videos. If not specified, it will be inferred from the dataset.\")\n",
    "parser.add_argument(\"--max_frames\", type=int, default=20)\n",
    "parser.add_argument(\"--max_latent_frames\", type=int, default=10, help=\"Number of frames to sample in each stage. Defaults to max_frames/2.\")\n",
    "parser.add_argument(\"--start_index\", type=int, default=0)\n",
    "parser.add_argument(\"--stop_index\", type=int, default=1)\n",
    "parser.add_argument(\"--sampler\", type=str, default=\"heun-80-inf-0-1-1000-0.002-7-100\")\n",
    "parser.add_argument(\"--use_ddim\", type=str2bool, default=False)\n",
    "parser.add_argument(\"--eval_on_train\", type=str2bool, default=True)\n",
    "parser.add_argument(\"--timestep_respacing\", type=str, default=\"\")\n",
    "parser.add_argument(\"--clip_denoised\", type=str2bool, default=False, help=\"If true, diffusion model generates data between [-1,1].\")\n",
    "parser.add_argument(\"--optimality\", type=str, default=None, choices=[\"linspace-t\", \"random-t\", \"linspace-t-force-nearby\", \"random-t-force-nearby\"],\n",
    "                    help=\"Type of optimised sampling scheme to use for choosing observed frames. By default uses non-optimized sampling scheme. The optimal indices should be computed before use via video_optimal_schedule.py.\")\n",
    "parser.add_argument(\"--device\", default=\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "parser.add_argument(\"--add_gt\", type=str2bool, default=True)\n",
    "parser.add_argument(\"--do_n\", type=int, default=2)\n",
    "parser.add_argument(\"--n_seeds\", type=int, default=2)\n",
    "parser.add_argument(\"--format\", type=str, default=\"gif\", choices=[\"gif\", \"mp4\", \"avi\"])\n",
    "\n",
    "parser.add_argument(\"--eval_dataset_config\", type=str, default=eval_dataset_configs[\"default\"], choices=list(eval_dataset_configs.keys()))\n",
    "parser.add_argument(\"--lower_frame_range\", type=int, default=0, help=\"Lower bound of frame index used for SpacedDatasets.\")\n",
    "parser.add_argument(\"--upper_frame_range\", type=int, default=None, help=\"Upper bound of frame index used for SpacedDatasets.\")\n",
    "parser.add_argument(\"--decode_chunk_size\", type=int, default=4)\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERRIDE ARGS HERE\n",
    "# args.checkpoint_path = \"checkpoints/wfbmfhok/ema_0.9999_900000.pt\"\n",
    "# args.checkpoint_path = \"checkpoints/0iz7ck2d/ema_0.9999_1050000.pt\"\n",
    "args.checkpoint_path = \"checkpoints/0iz7ck2d/ema_0.9999_1800000.pt\"\n",
    "args.clip_denoised = False\n",
    "args.out_dir = \"tmp/offline-1800000\"\n",
    "args.do_n = 4\n",
    "args.stop_index = 4\n",
    "args.T = 50\n",
    "args.format = \"mp4\"\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.indices = list(range(args.start_index, args.stop_index))\n",
    "if args.num_sampled_videos is None:\n",
    "    args.num_sampled_videos = len(args.indices)\n",
    "print(f\"Sampling for indices {args.start_index} to {args.stop_index}.\")\n",
    "\n",
    "# Load the checkpoint (state dictionary and config)\n",
    "data = dist_util.load_state_dict(args.checkpoint_path, map_location=\"cpu\")\n",
    "state_dict = data[\"state_dict\"]\n",
    "model_args = data[\"config\"]\n",
    "model_args.update({\"use_ddim\": args.sampler == \"ddim\",\n",
    "                    \"timestep_respacing\": args.timestep_respacing})\n",
    "model_args[\"diffusion_space_kwargs\"][\"enable_decoding\"] = True\n",
    "model_args = argparse.Namespace(**model_args)\n",
    "print(model_args)\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "    **args_to_dict(model_args, model_and_diffusion_defaults().keys())\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(args.device)\n",
    "model.eval()\n",
    "args.image_size = model_args.image_size\n",
    "if args.max_frames is None:\n",
    "    args.max_frames = model_args.max_frames\n",
    "if args.max_latent_frames is None:\n",
    "    args.max_latent_frames = args.max_frames // 2 \n",
    "if model_args.diffusion_space == \"latent\":\n",
    "    args.clip_denoised = False\n",
    "\n",
    "# Prepare samples directory\n",
    "args.eval_dir = get_model_results_path(args) / get_eval_run_identifier(args)\n",
    "samples_prefix = \"samples\"\n",
    "\n",
    "# Load the dataset (to get observations from)\n",
    "eval_dataset_args = dict(dataset_name=model_args.dataset, T=args.T, train=args.eval_on_train,\n",
    "                         eval_dataset_config=args.eval_dataset_config)\n",
    "# if args.eval_dataset_config == eval_dataset_configs[\"default\"]:\n",
    "if args.eval_dataset_config != eval_dataset_configs[\"continuous\"]:\n",
    "    spacing_kwargs = dict(n_data=args.num_sampled_videos,\n",
    "                          frame_range=(args.lower_frame_range, args.upper_frame_range))\n",
    "    eval_dataset_args[\"spacing_kwargs\"] = spacing_kwargs\n",
    "dataset = get_eval_dataset(**eval_dataset_args)\n",
    "\n",
    "\n",
    "(args.eval_dir / samples_prefix).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving samples to {args.eval_dir / samples_prefix}\")\n",
    "\n",
    "# Store model configs in a JSON file\n",
    "json_path = args.eval_dir / \"model_config.json\"\n",
    "if not json_path.exists():\n",
    "    with Protect(json_path): # avoids race conditions\n",
    "        to_save = vars(model_args)\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(to_save, f, indent=4)\n",
    "    print(f\"Saved model config at {json_path}\")\n",
    "\n",
    "all_samples_and_seeds = []\n",
    "all_unprocessed_samples_and_seeds = []\n",
    "for seed in range(args.n_seeds):\n",
    "    all_samples, all_unprocessed_samples = generate_samples(args, model, diffusion, dataset, samples_prefix, seed=seed, cache_samples=True)\n",
    "    all_samples_and_seeds.append(all_samples)\n",
    "    all_unprocessed_samples_and_seeds.append(all_unprocessed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples_and_seeds[0][0].shape\n",
    "print(len(all_samples_and_seeds[0]))\n",
    "print(len(all_samples_and_seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from improved_diffusion.test_util import mark_as_observed, tensor2gif, tensor2mp4\n",
    "from improved_diffusion.test_util import parse_eval_run_identifier\n",
    "import uuid\n",
    "\n",
    "\n",
    "parsed = parse_eval_run_identifier(os.path.basename(args.eval_dir))\n",
    "T, obs_length, eval_on_train = parsed[\"T\"], parsed[\"n_obs\"], parsed[\"eval_on_train\"]\n",
    "eval_dataset_config = parsed[\"eval_dataset_config\"]\n",
    "if eval_dataset_config == eval_dataset_configs[\"default\"]:\n",
    "    assert args.num_sampled_videos is not None, \"You must specified how many videos were generated by video_sample.py if not using visualization mode.\"\n",
    "\n",
    "samples_prefix, videos_prefix = \"samples\", \"videos\"\n",
    "if args.add_gt:\n",
    "    try:  # Infer T from sampled video\n",
    "        T = len(np.load(Path(args.eval_dir) / samples_prefix / f\"sample_{args.start_index:04d}-0.npy\"))\n",
    "    except PermissionError:\n",
    "        T = None\n",
    "    model_args_path = Path(args.eval_dir) / \"model_config.json\"\n",
    "    with open(model_args_path, \"r\") as f:\n",
    "        model_args = argparse.Namespace(**json.load(f))\n",
    "\n",
    "    # Load the dataset (to get observations from)\n",
    "    eval_dataset_args = dict(dataset_name=model_args.dataset, T=T,\n",
    "                                train=eval_on_train, eval_dataset_config=eval_dataset_config)\n",
    "    if eval_dataset_config == eval_dataset_configs[\"default\"]:\n",
    "        spacing_kwargs = dict(n_data=args.num_sampled_videos,\n",
    "                                frame_range=(parsed[\"lower_frame_range\"], parsed[\"upper_frame_range\"]))\n",
    "        eval_dataset_args[\"spacing_kwargs\"] = spacing_kwargs\n",
    "    dataset = get_eval_dataset(**eval_dataset_args)\n",
    "\n",
    "out_dir = (Path(args.out_dir) if args.out_dir is not None else Path(args.eval_dir)) / videos_prefix\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / f\"{args.do_n}_{args.n_seeds}.{args.format}\"\n",
    "\n",
    "videos = []\n",
    "decode_fn = None\n",
    "for data_idx in range(args.start_index, args.start_index+args.do_n):\n",
    "    if args.add_gt:\n",
    "\n",
    "        gt_drange = [-1, 1]\n",
    "        gt_video, _ = dataset[data_idx]\n",
    "\n",
    "        if gt_video.shape[-3] == 4:\n",
    "            if decode_fn is None:\n",
    "                model_args.diffusion_space_kwargs[\"enable_decoding\"] = True\n",
    "                diffusion = create_model_and_diffusion(**args_to_dict(model_args, model_and_diffusion_defaults().keys()))[1]\n",
    "                decode_fn = diffusion.decode\n",
    "            decode_input = gt_video.to(th.float16).unsqueeze(0)\n",
    "            gt_video = decode_fn(decode_input, chunk_size=args.decode_chunk_size).to(gt_video.device).squeeze()\n",
    "\n",
    "        gt_video = (gt_video.clamp(*gt_drange).numpy() - gt_drange[0]) / (gt_drange[1] - gt_drange[0])  * 255\n",
    "        gt_video = gt_video.astype(np.uint8)\n",
    "        mark_as_observed(gt_video)\n",
    "        videos.append([gt_video])\n",
    "    else:\n",
    "        videos.append([])\n",
    "    seed = 0\n",
    "    done = 0\n",
    "    while done < args.n_seeds:\n",
    "        filename = Path(args.eval_dir) / samples_prefix / f\"sample_{data_idx:04d}-{seed}.npy\"\n",
    "        print(filename)\n",
    "        try:\n",
    "            video = np.load(filename)\n",
    "            mark_as_observed(video[:obs_length])\n",
    "            videos[-1].append(video)\n",
    "            done += 1\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        seed += 1\n",
    "        assert seed < 100, f'Not enough seeds for idx {data_idx} (found {done} after trying {seed} seeds)'\n",
    "    videos[-1] = np.concatenate(videos[-1], axis=-2)\n",
    "video = np.concatenate(videos, axis=-1)\n",
    "\n",
    "random_str = uuid.uuid4()\n",
    "if args.format == \"gif\":\n",
    "    tensor2gif(th.tensor(video), out_path, drange=[0, 255], random_str=random_str)\n",
    "elif args.format == \"mp4\":\n",
    "    print(th.tensor(video).shape, th.tensor(video).dtype)\n",
    "    tensor2mp4(th.tensor(video), out_path, drange=[0, 255], random_str=random_str)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown format {args.format}\")\n",
    "print(f\"Saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# display(Image(data=open(out_path,'rb').read(), format='png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
