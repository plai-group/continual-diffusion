import torch as th
import numpy as np
from argparse import ArgumentParser
from pathlib import Path
import uuid
import argparse
import json
import os

from improved_diffusion.video_datasets import get_eval_dataset, eval_dataset_configs
from improved_diffusion.test_util import mark_as_observed, tensor2gif, tensor2mp4, parse_eval_run_identifier
from improved_diffusion.script_util import str2bool, create_model_and_diffusion, model_and_diffusion_defaults, args_to_dict

"""
Sample Command
python scripts/video_make_mp4.py --eval_dir=results/p9lrebju/ema_0.9999_050000_heun-80-inf-0-1-1000-0.002-7-100/autoreg_10_5_50_5 --do_n=3 --obs_length=5 --eval_on_train=True --num_sampled_videos=3 --add_gt=False
"""


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("--eval_dir", type=str, required=True)
    parser.add_argument("--num_sampled_videos", type=int, default=None, help="Specify how many videos were generated by video_sample.py.")
    parser.add_argument("--out_dir", type=str, default=None)
    parser.add_argument("--add_gt", type=str2bool, default=True)
    parser.add_argument("--do_n", type=int, default=1)
    parser.add_argument("--start_idx", type=int, default=0)
    parser.add_argument("--n_seeds", type=int, default=1)
    parser.add_argument("--decode_chunk_size", type=int, default=10)
    parser.add_argument("--format", type=str, default="gif", choices=["gif", "mp4", "avi"])
    args = parser.parse_args()

    parsed = parse_eval_run_identifier(os.path.basename(args.eval_dir))
    T, obs_length, eval_on_train = parsed["T"], parsed["n_obs"], parsed["eval_on_train"]
    eval_dataset_config = parsed["eval_dataset_config"]
    if eval_dataset_config == eval_dataset_configs["default"]:
        assert args.num_sampled_videos is not None, "You must specified how many videos were generated by video_sample.py if not using visualization mode."

    samples_prefix, videos_prefix = "samples", "videos"
    if args.add_gt:
        try:  # Infer T from sampled video
            T = len(np.load(Path(args.eval_dir) / samples_prefix / f"sample_{args.start_idx:04d}-0.npy"))
        except PermissionError:
            T = None
        model_args_path = Path(args.eval_dir) / "model_config.json"
        with open(model_args_path, "r") as f:
            model_args = argparse.Namespace(**json.load(f))

        # Load the dataset (to get observations from)
        eval_dataset_args = dict(dataset_name=model_args.dataset, T=T,
                                 train=eval_on_train, eval_dataset_config=eval_dataset_config)
        if eval_dataset_config == eval_dataset_configs["default"]:
            spacing_kwargs = dict(n_data=args.num_sampled_videos,
                                  frame_range=(parsed["lower_frame_range"], parsed["upper_frame_range"]))
            eval_dataset_args["spacing_kwargs"] = spacing_kwargs
        dataset = get_eval_dataset(**eval_dataset_args)

    out_dir = (Path(args.out_dir) if args.out_dir is not None else Path(args.eval_dir)) / videos_prefix
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / f"{args.do_n}_{args.n_seeds}.{args.format}"

    videos = []
    decode_fn = None
    for data_idx in range(args.start_idx, args.start_idx+args.do_n):
        if args.add_gt:

            gt_drange = [-1, 1]
            gt_video, _ = dataset[data_idx]

            if gt_video.shape[-3] == 4:
                if decode_fn is None:
                    model_args.diffusion_space_kwargs["enable_decoding"] = True
                    diffusion = create_model_and_diffusion(**args_to_dict(model_args, model_and_diffusion_defaults().keys()))[1]
                    decode_fn = diffusion.decode
                decode_input = gt_video.to(th.float16).unsqueeze(0)
                gt_video = decode_fn(decode_input, chunk_size=args.decode_chunk_size).to(gt_video.device).squeeze()

            gt_video = (gt_video.clamp(*gt_drange).numpy() - gt_drange[0]) / (gt_drange[1] - gt_drange[0])  * 255
            gt_video = gt_video.astype(np.uint8)
            mark_as_observed(gt_video)
            videos.append([gt_video])
        else:
            videos.append([])
        seed = 0
        done = 0
        while done < args.n_seeds:
            filename = Path(args.eval_dir) / samples_prefix / f"sample_{data_idx:04d}-{seed}.npy"
            print(filename)
            try:
                video = np.load(filename)
                mark_as_observed(video[:obs_length])
                videos[-1].append(video)
                done += 1
            except PermissionError:
                pass
            seed += 1
            assert seed < 100, f'Not enough seeds for idx {data_idx} (found {done} after trying {seed} seeds)'
        videos[-1] = np.concatenate(videos[-1], axis=-2)
    video = np.concatenate(videos, axis=-1)

    random_str = uuid.uuid4()
    if args.format == "gif":
        tensor2gif(th.tensor(video), out_path, drange=[0, 255], random_str=random_str)
    elif args.format == "mp4":
        print(th.tensor(video).shape, th.tensor(video).dtype)
        tensor2mp4(th.tensor(video), out_path, drange=[0, 255], random_str=random_str)
    else:
        raise ValueError(f"Unknown format {args.format}")
    print(f"Saved to {out_path}")
